Formas de import do Python
import torch
from torch import nn
import torch.nn.functional as F
from torchvision import datasets, transforms

MODEL de nn
+ Módelo da Rde Neural do pacte `nn`
+ init__
  
+ forward__
+ fazer dropout
  - em init
  - em forward



TENSOR

+ TENSOR.shape : retorna dimensôes
+ ALterar shape do TENSOR: Há 3 formas
  + `weights.reshape(a, b)`
    - will return a new tensor with the same data as `weights` with size `(a, b)` sometimes, and sometimes a clone, as in it copies the data to another part of memory.
  + `weights.resize_(a, b)`: se a nova dimensão for muito difernete, pode-se perder/adicionar dados
    - returns the same tensor with a different shape. However, if the new shape results in fewer elements than the original tensor, some elements will be removed from the tensor (but not from memory). If the new shape results in more elements than the original tensor, new elements will be uninitialized in memory. Here I should note that the underscore at the end of the method denotes that this method is performed **in-place**.
  + `weights.view(a, b)` : a mais segura
    - will return a new tensor with the same data as `weights` with size `(a, b)`

módulo `nn`
+ Módulo que fornece RedesNeurais

módulo `autograd`
+ Automatizador de gradientes (só calcula, nâo atualisa os pesos)
+ Autograd works by keeping track of operations performed on tensors, then going backwards through those operations, calculating gradients along the way. To make sure PyTorch keeps track of operations on a tensor and calculates the gradients, you need to set requires_grad = True on a tensor.
+ Verificar se tá com grad
  - Por Default, um tensor não habilita um o grad. E
  - Para ativar grad:
    - x = torch.zeros(1, requires_grad=True)
  - Para desligar/ligar num TENSOR já existente
    - torch.set_grad_enabled(True|False)
  - Desligar num bloco:
    - with torch.no_grad(): // Os tensors internos estarão desligados
+ Rodar backPropagation
  - Sobre o Tensor final você aplica `backward()`. O PyTorch salva as operações feitas nos tensors e em quais foram. Fazendo isso, vai calcular a derivada das operações que origiaram esse valor final e mudar esses tensors
  - Então, numa rede neural, calculariamos o `backward()` no erro
+ ````
  with torch.no_grad(): # Turn off gradients to speed up this part
    logps = model(img)
  ````
  - Muitas vezes, usamos o no_grad para agilizar a aplicaçÂo da rede (quando vamos testar, já que, ao executar a rede é semples calculado o gradiente, a nao ser que coloque no bloco no_grad()

módulo `optim`
+ Otimizador para atualizar os pesos com o gradiente
+ `from torch import optim`
+ Exemplo Otimizador,por default a variaável se chama optimizer : `optimizer = optim.....`
  - Exemplo
  - optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
  -optimizer = optim.Adam([var1, var2], lr=0.0001)
+ `optim.SGD`
  + SINTAX: orch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)
  + Implements stochastic gradient descent (optionally with momentum).
  + Exemplo: `optimizer = optim.SGD(model_of_nn.parameters(), lr=0.01)`
+ optimizer.step()
  - All optimizers implement a step() method, that updates the parameters. It can be used in two ways
  - É você dar o passo, atualizar os pesos
+ optimizer.zero_grad() # Zerar o otimizador. Deve ser feito sempre

MULTIPLICAR MATRIZES
torch.mm || torch.matmul()

CONVERTER TENSOR DE BOOLEAN PARA FLOAT/INT
TENSOR.type(torch.FloatTensor)

BUSCAR INDEX DE ELEMENTO EM UM TENSOR
+ SNIPPET:
 - (TENSOR == 2).nonzero())
   - Retorna Tensor que tem a primeira posição do elemento (???Nâo sei)

MELHORAR A FORMATAÇÂO DO PRINT DE UM TENSOR

+ torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None, sci_mode=None)
 - Exemplo: torch.set_printoptions(precision=10)
   - VOcê seta uma melhor precisão

+ Arredondar TENSOR
 - SNIPPET:
 - ````
	n_digits = 3
	rounded = torch.round(arr * 10**n_digits) / (10**n_digits)
   ````


NOTAÇÕES
+ TENSOR = A variável que é um Tensor

+ torch.randn(): Gera números aleatórios entre 0 e 1
  - O parâmerto é para especificar a dimensão do tensor
  - (1,5) : row/coluns
  - features = torch.randn((1, 5))
  - > creates a tensor with shape `(1, 5)`, one row and five columns, that contains values randomly distributed according to the normal distribution with a mean of zero and standard deviation of one. 
  - tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]])
  - torch.randn((1, 1))
    - tensor([[0.3177]])

+ torch.randn_like()
  - Serve para retornar a mesma coisa que `torch.randn()` mas com a dimensão no formato do tensor passado
  - > creates another tensor with the same shape as `features`, again containing values from a normal distribution.
  - Exemplo: `torch.randn_like(features)` vai retornar um tensor da mesma dimensâo de `features`

+ torch.sum ou TENSOR.sum()
  + Soma todos os elementos de um array num elemento só.
  + Facilita somatório de matriz
  + Exemplo: 
    - features = tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]])
  = features.sum() = tensor(2.1626)


+ torch.mm(TENSOR1, TENSOR 2)
  - SINTAX: `torch.mm(input, mat2, out=None) → Tensor`
    - input (Tensor) – the first matrix to be multiplied
    - mat2 (Tensor) – the second matrix to be multiplied
    - out (Tensor, optional) – the output tensor.
  - Faz MUtiplicação de Matrizes. 
  - Lembre-se que, como vimos em algebra linear ao multiplciar matrizes ** O número de Colunas da Primeira Matriz deve ser Igual ao Número de LInhas da Segunda Matriz
  - Assim, se eu tenho
    + M1(a,b) e M2(c,d), para multiplicar matricial, **b == c**
  + If input is a (n \times m)(n×m) tensor, mat2 is a (m \times p)(m×p) tensor, out will be a (n \times p)(n×p) tensor.

