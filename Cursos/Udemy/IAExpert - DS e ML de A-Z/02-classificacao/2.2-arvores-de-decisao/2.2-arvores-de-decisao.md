# Árvore de Decisão

LINK: 
+ https://www.datacamp.com/community/tutorials/decision-tree-classification-python

LEITURAS

Livro Data Mining with Decision Trees: Theory and Applications (Machine Perception and Artificial Intelligence) de Oded Z. Maimon: livro de fácil compreensão e focado somente em árvores de decisão

Livro C4.5: Programs for Machine Learning (Morgan Kaufmann Series in Machine Learning) de J. Ross Quinlan: um dos livros mais clássicos sobre o assunto

Livro Decision Tress and Random Forests: A Visual Introduction For Begginers: A Simple Guide to Machine Learning With Decision Trees de Chris Smith e Mark Koning: um dos livros mais fáceis e didáticos sobre o assunto

Livro Machine Learning With Random Forests And Decision Trees: A Visual Guide For Beginners de Scott Hartshorn: outro livro também muito didático 

Artigo Mineração de Dados com Árvores de Decisão de Jones Granatyr, Fábio Spak, Fabrício Enembreck e Otto Robert Lessing: artigo que fizemos para a revista Devmedia, que mostra a teoria a implementação prática de árvores de decisão no software Weka

## O que é Árvore de Decisão
Entropia:
+ Você calcula a entropia da feature, que dá um valor 

Ganho de Informação
+ Define a importância de um atributo para a classificação
+ Utiliza o cálculo de entropia para calcula a da feaures e a dos valores
+ Você calcula a entropia dos valores de uma features para determinar o ganho de informação, ou seja, o quão bem essa feature SOZINHA é capaz de dividir a base
+ Quanto maior for o ganho de informaçÂo de uma features, quer dizer que ela divide melhor os dados, entâo, deve está no topo da árvore de decisão

O processo:
+ Você calcula entropia várias e várias vezes

## Prós-Contras

Vantagens
+ Fácil interpretação
+ Nâo precisa de normalização
+ Rápida em classificar registros

Desvantagens
+ Pode gerar árvores muito complexas o que pode gerar overfiting
+ Mudnaças na árvore gera mudança bruscas nas árvores
+ Problema NP-Completo para construir a árvore

+ Eram muito populares nos anos 90.
+ UPGRADE: RandomForest melhoram o desempenho
+ CART:


Pros
Decision trees are easy to interpret and visualize.
It can easily capture Non-linear patterns.
It requires fewer data preprocessing from the user, for example, there is no need to normalize columns.
It can be used for feature engineering such as predicting missing values, suitable for variable selection.
The decision tree has no assumptions about distribution because of the non-parametric nature of the algorithm. (Source)
Cons
Sensitive to noisy data. It can overfit noisy data.
The small variation(or variance) in data can result in the different decision tree. This can be reduced by bagging and boosting algorithms.
Decision trees are biased with imbalance dataset, so it is recommended that balance out the dataset before creating the decision tree.
