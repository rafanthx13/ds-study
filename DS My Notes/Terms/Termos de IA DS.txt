
### Neural Network:
+ FeedForward
+ BackPropagation
+ Gradient Descent (Descida do gradiente)
  - Busca a direção que desce mais a função. Como ela pode parar quando encontra mínimos locais, então é bom começar de vários pontos diferentes, isso aumenta a chance de achar o mínimo global
+ Bias

### Métricas de Erro
MeanSaquareError(MSE)
+ Para problemas de clasificação
  - É fazer uma média do somatório de (a classe que deveria ser - a prob. dada para essa classe.
+ Exemplo: Se a classe que deveria dar a tag é 0, e calculou 0.406,então faresmo (0 - 0.406)^2. Fazer isso com todas as previsoes, somar tudo e dividir pela quantidade (MÉDIA)


RootMeanError(RMSE)
- É o mesmo que o anterior mas com a raiz quadrado do resultado final


### Termos Gerais
+ Epochs
 - Unidade de treinamento da rede. Calcular a previsâo, verificar o erros e alterar os pesos pelo BackPropagation
+ HyperParametros: Parametro como (learning-rate (lr))
+ Transfer Learning (TL). Usar uma rede treinada para um outro problema, asism, é passado a aprendizagem de um problema para outro. COnsiste entãoe em: Usar modelos já bons e outros problemas diferentes. É a reutilização de um modelo pré-treinado em um novo problema. Isto é, vou usar uma rede neural treinada em outro outro conjunto de dados, geralmente maior, para resolver um novo problema.

### Otimização
+ CUDA
  - Quer dizer "Compute Unified Device Architecture" desenvolvida pela Nvidea. Serve para computação paralela, ou seja, para otimizar a execução de um código utilizando GPU.

### Step Functions
+ Sigmoid Function
  - y entre [0,1]. O intervalo é de 1
+ Hyper Bolian Tangent Function
  - y é entre [-1,1]. O intervalo é de 2
+ SoftMax function
  - Boa para distribuiçâo de probabildiade (para classificação entre mais de duas classificações)
+ Rectified Linear Unit (ReLu)
  - Uma reta positiva que vai de [0, inf]. Se valor positivo, retorna esse avlor, se neagito, retorna 0.


### Representation of Labels
+ One-Hot Coding:

### Estatistica
+ Maximun Likelihood

### Logaritmo
+ Log(ab) = log(a) + log(b). Lembre-se que assim, podemos tranforma um produtório em um somatório, por logaritmo ser estritramente crescente (quando tudo tem a mesma base)

### Random
+ Cross-Entropy

### NumPy

np.dot(x,y): Multiplicaçâo de matrizes
np.exp(x): E (número de Euler) elevado a 'x'
np.log(x): log do número

15/08: 14:25


